{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6518a415-9c3a-4d62-bd7e-a637115e103c",
   "metadata": {},
   "source": [
    "Setup and Imports : This block includes necessary library imports, optimizations, and configuration for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16f73aa-23fb-4235-b5b6-ea0abb8ee31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: intel-extension-for-pytorch in /home/dev/conda/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: scikit-learn in /home/dev/conda/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: psutil in /home/dev/conda/lib/python3.10/site-packages (from intel-extension-for-pytorch) (6.0.0)\n",
      "Requirement already satisfied: numpy in /home/dev/conda/lib/python3.10/site-packages (from intel-extension-for-pytorch) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/dev/conda/lib/python3.10/site-packages (from intel-extension-for-pytorch) (24.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/dev/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/dev/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dev/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install intel-extension-for-pytorch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab81e87d-74df-454d-8c1c-874ad2f3c822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1313/3032103304.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Scaler for mixed precision\n",
      "/home/dev/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "import timm\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import time\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from neural_compressor import quantization\n",
    "# from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "# Enable Intel MKL-DNN optimizations for CPU if using Intel CPU\n",
    "torch.backends.mkldnn.enabled = True\n",
    "\n",
    "# Enable mixed precision training (FP16) if using CUDA (GPU)\n",
    "use_amp = torch.cuda.is_available()  # Check if CUDA is available for mixed precision\n",
    "scaler = GradScaler()  # Scaler for mixed precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace5ea3-2210-40b9-bf80-fe16e0691ec6",
   "metadata": {},
   "source": [
    "Directory setup : Concatinating the data here for use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a547aa2-7301-4eac-95a9-a76f44170bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'benign': 0, 'malignant': 1}\n",
      "Benign images: 401059\n",
      "Malignant images: 11040\n",
      "Training samples: 17664, Validation samples: 4416\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"/workspace/2024/image/\", transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(dataset.class_to_idx)\n",
    "\n",
    "benign_dir = \"/workspace/2024/image/benign/\"\n",
    "malignant_dir = \"/workspace/2024/image/malignant/\"\n",
    "\n",
    "benign_paths = [os.path.join(benign_dir, f) for f in os.listdir(benign_dir) if f.endswith(('.jpg', '.png'))]\n",
    "malignant_paths = [os.path.join(malignant_dir, f) for f in os.listdir(malignant_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "print(f\"Benign images: {len(benign_paths)}\")\n",
    "print(f\"Malignant images: {len(malignant_paths)}\")\n",
    "\n",
    "benign_sample = benign_paths[:len(malignant_paths)]\n",
    "balanced_paths = benign_sample + malignant_paths\n",
    "labels = [0] * len(benign_sample) + [1] * len(malignant_paths)\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    balanced_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_paths)}, Validation samples: {len(val_paths)}\")\n",
    "\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/train/benign\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/train/malignant\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/val/benign\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/val/malignant\", exist_ok=True)\n",
    "\n",
    "for path, label in zip(train_paths, train_labels):\n",
    "    target_dir = \"/workspace/2024/vit/dataset/train/benign\" if label == 0 else \"/workspace/2024/vit/dataset/train/malignant\"\n",
    "    # shutil.copy(path, target_dir)\n",
    "for path, label in zip(val_paths, val_labels):\n",
    "    target_dir = \"/workspace/2024/vit/dataset/val/benign\" if label == 0 else \"/workspace/2024/vit/dataset/val/malignant\"\n",
    "    # shutil.copy(path, target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cd2d5-5f11-4306-885d-872dfba01029",
   "metadata": {},
   "source": [
    "Data Transformation and DataLoader Setup : This block prepares the data pipeline for training and validation, with augmentation and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28716718-2f52-4adc-b1d6-26395ba2844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['benign', 'malignant'], Class to Index: {'benign': 0, 'malignant': 1}\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Higher resolution for better performance\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader with optimization\n",
    "train_dataset = datasets.ImageFolder(root=\"/workspace/2024/vit/dataset/train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"/workspace/2024/vit/dataset/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    pin_memory=True, \n",
    "    prefetch_factor=2, \n",
    "    persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Print the class names and mappings\n",
    "print(f\"Classes: {train_dataset.classes}, Class to Index: {train_dataset.class_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eeee10-a7dc-4cf5-b051-fe6856160b34",
   "metadata": {},
   "source": [
    "Define the Training Function : This block defines the train_model function, which handles the training, validation, and early stopping logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104c9307-dd22-40a6-964a-28235acd03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping and checkpointing\n",
    "def train_model(model, train_loader, val_loader, epochs=1, patience=5, checkpoint_dir=\"/workspace/2024/vit/checkpoints\"):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')  # To track the best validation loss\n",
    "    epochs_without_improvement = 0  # To track patience\n",
    "    best_model_wts = None  # To save the best model weights\n",
    "\n",
    "    # Ensure checkpoint directory exists\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()  # Start time for this epoch\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision context if using GPU\n",
    "            with autocast(enabled=use_amp):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            # Scale gradients and accumulate for larger batch sizes\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Update weights every `accumulation_steps`\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += targets.size(0)\n",
    "            correct_train += (predicted == targets).sum().item()\n",
    "\n",
    "        # Compute average training loss and accuracy for the epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Check if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_wts = model.state_dict()  # Save the best model weights\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "        # Early stopping if no improvement for `patience` epochs\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "    # Load best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(\"Restored the best model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06ba2a-4eea-489f-b229-557b5549dd91",
   "metadata": {},
   "source": [
    "Validation Function : This block contains the validate_model function that computes the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322cf4b3-20e9-4828-93ec-fe3cca01b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606bd31-746c-4416-8392-0ec531b6c267",
   "metadata": {},
   "source": [
    "Delete Cache : Clears the CPU memory by deleting unused variables and calling garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942b892a-7ad2-4323-bc78-5b55d88aefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cpu_memory():\n",
    "    gc.collect()  # Forces Python's garbage collection to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb4a4a-96b3-4928-afea-13fc55e546aa",
   "metadata": {},
   "source": [
    "Model Initialization and Training Loop : This block trains each model in sequence, applying Intel optimizations, training, quantization, and JIT scripting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd7fbf9-cfc5-4624-b341-15c9c980f2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1313/85315055.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 1453) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m vit_tiny\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit_tiny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# # Post-training Quantization using Neural Compressor\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"Quantizing vit_tiny_patch16_224...\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# config = PostTrainingQuantConfig()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Apply JIT scripting for deployment\u001b[39;00m\n\u001b[1;32m     25\u001b[0m script_vit_tiny \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript(vit_tiny)\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, patience, checkpoint_dir)\u001b[0m\n\u001b[1;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Scale gradients and accumulate for larger batch sizes\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Update weights every `accumulation_steps`\u001b[39;00m\n\u001b[1;32m     35\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/conda/lib/python3.10/warnings.py:403\u001b[0m, in \u001b[0;36mWarningMessage.__init__\u001b[0;34m(self, message, category, filename, lineno, file, line, source)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWarningMessage\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    400\u001b[0m     _WARNING_DETAILS \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    401\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, message, category, filename, lineno, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    404\u001b[0m                  line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m=\u001b[39m category\n",
      "File \u001b[0;32m~/conda/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 1453) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit."
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model initialization for vit_tiny_patch16_224\n",
    "vit_tiny = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=2)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "vit_tiny, optimizer = ipex.optimize(vit_tiny, optimizer=Adam(vit_tiny.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "vit_tiny.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(vit_tiny, train_loader, val_loader, epochs=1, patience=5)\n",
    "\n",
    "# # Post-training Quantization using Neural Compressor\n",
    "# print(\"Quantizing vit_tiny_patch16_224...\")\n",
    "# config = PostTrainingQuantConfig()\n",
    "# quantized_vit_tiny = quantization.fit(vit_tiny, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# # Save the quantized model\n",
    "# torch.save(quantized_vit_tiny.state_dict(), '/workspace/2024/image/quantized_model/vit_tiny_patch16_224_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "script_vit_tiny = torch.jit.script(vit_tiny)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(script_vit_tiny, '/workspace/2024/vit/scripted_model/vit_tiny_patch16_224_scripted.pt')\n",
    "\n",
    "print(\"Training completed for vit_tiny_patch16_224.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd1f6b-b50d-42ee-b2ab-a8d15b785bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization for vit_small_patch16_224\n",
    "vit_small = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "vit_small, optimizer = ipex.optimize(vit_small, optimizer=Adam(vit_small.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "vit_small.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(vit_small, train_loader, val_loader, epochs=50, patience=5)\n",
    "\n",
    "# Post-training Quantization using Neural Compressor\n",
    "print(\"Quantizing vit_small_patch16_224...\")\n",
    "config = PostTrainingQuantConfig()\n",
    "quantized_vit_small = quantization.fit(vit_small, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_vit_small.state_dict(), '/workspace/2024/image/quantized_model/vit_small_patch16_224_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "quantized_vit_small = torch.jit.script(quantized_vit_small)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(quantized_vit_small, '/workspace/2024/image/scripted_model/vit_small_patch16_224_quantized_scripted.pt')\n",
    "\n",
    "print(\"Training and quantization completed for vit_small_patch16_224.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268882e-1894-474c-9e51-9ab5b7992071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization for vit_base_patch16_224\n",
    "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "vit_base, optimizer = ipex.optimize(vit_base, optimizer=Adam(vit_base.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "vit_base.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(vit_base, train_loader, val_loader, epochs=50, patience=5)\n",
    "\n",
    "# Post-training Quantization using Neural Compressor\n",
    "print(\"Quantizing vit_base_patch16_224...\")\n",
    "config = PostTrainingQuantConfig()\n",
    "quantized_vit_base = quantization.fit(vit_base, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_vit_base.state_dict(), '/workspace/2024/image/quantized_model/vit_base_patch16_224_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "quantized_vit_base = torch.jit.script(quantized_vit_base)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(quantized_vit_base, '/workspace/2024/image/scripted_model/vit_base_patch16_224_quantized_scripted.pt')\n",
    "\n",
    "print(\"Training and quantization completed for vit_base_patch16_224.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b57c8-aa24-4477-a3ca-d92b07fb0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization for convnextv2_nano\n",
    "convnextv2 = timm.create_model('convnextv2_nano', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "convnextv2, optimizer = ipex.optimize(convnextv2, optimizer=Adam(convnextv2.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "convnextv2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(convnextv2, train_loader, val_loader, epochs=50, patience=5)\n",
    "\n",
    "# Post-training Quantization using Neural Compressor\n",
    "print(\"Quantizing convnextv2_nano...\")\n",
    "config = PostTrainingQuantConfig()\n",
    "quantized_convnextv2 = quantization.fit(convnextv2, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_convnextv2.state_dict(), '/workspace/2024/image/quantized_model/convnextv2_nano_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "quantized_convnextv2 = torch.jit.script(quantized_convnextv2)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(quantized_convnextv2, '/workspace/2024/image/scripted_model/convnextv2_nano_quantized_scripted.pt')\n",
    "\n",
    "print(\"Training and quantization completed for convnextv2_nano.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b94cad-cfbc-4b99-88f8-0ab5f892d47d",
   "metadata": {},
   "source": [
    "Final Message : This final block confirms that all models have been trained and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619199c4-1e5c-4748-9d8d-fb1d0cf3d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "print(\"All models trained, quantized, and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
