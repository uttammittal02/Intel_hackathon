{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6518a415-9c3a-4d62-bd7e-a637115e103c",
   "metadata": {},
   "source": [
    "Setup and Imports : This block includes necessary library imports, optimizations, and configuration for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16f73aa-23fb-4235-b5b6-ea0abb8ee31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: intel-extension-for-pytorch in /home/dev/conda/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: scikit-learn in /home/dev/conda/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: psutil in /home/dev/conda/lib/python3.10/site-packages (from intel-extension-for-pytorch) (6.0.0)\n",
      "Requirement already satisfied: numpy in /home/dev/conda/lib/python3.10/site-packages (from intel-extension-for-pytorch) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/dev/conda/lib/python3.10/site-packages (from intel-extension-for-pytorch) (24.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/dev/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/dev/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dev/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install intel-extension-for-pytorch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab81e87d-74df-454d-8c1c-874ad2f3c822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_956/1485081988.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Scaler for mixed precision\n",
      "/home/dev/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "import timm\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import time\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from neural_compressor import quantization\n",
    "# from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "# Enable Intel MKL-DNN optimizations for CPU if using Intel CPU\n",
    "torch.backends.mkldnn.enabled = True\n",
    "\n",
    "# Enable mixed precision training (FP16) if using CUDA (GPU)\n",
    "use_amp = torch.cuda.is_available()  # Check if CUDA is available for mixed precision\n",
    "scaler = GradScaler()  # Scaler for mixed precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace5ea3-2210-40b9-bf80-fe16e0691ec6",
   "metadata": {},
   "source": [
    "Directory setup : Concatinating the data here for use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a547aa2-7301-4eac-95a9-a76f44170bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'benign': 0, 'malignant': 1}\n",
      "Benign images: 401059\n",
      "Malignant images: 11040\n",
      "Training samples: 17664, Validation samples: 4416\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"/workspace/2024/image/\", transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(dataset.class_to_idx)\n",
    "\n",
    "benign_dir = \"/workspace/2024/image/benign/\"\n",
    "malignant_dir = \"/workspace/2024/image/malignant/\"\n",
    "\n",
    "benign_paths = [os.path.join(benign_dir, f) for f in os.listdir(benign_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "malignant_paths = [os.path.join(malignant_dir, f) for f in os.listdir(malignant_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "print(f\"Benign images: {len(benign_paths)}\")\n",
    "print(f\"Malignant images: {len(malignant_paths)}\")\n",
    "\n",
    "benign_sample = benign_paths[:len(malignant_paths)]\n",
    "balanced_paths = benign_sample + malignant_paths\n",
    "labels = [0] * len(benign_sample) + [1] * len(malignant_paths)\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    balanced_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_paths)}, Validation samples: {len(val_paths)}\")\n",
    "\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/train/benign\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/train/malignant\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/val/benign\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/2024/vit/dataset/val/malignant\", exist_ok=True)\n",
    "\n",
    "for path, label in zip(train_paths, train_labels):\n",
    "    target_dir = \"/workspace/2024/vit/dataset/train/benign\" if label == 0 else \"/workspace/2024/vit/dataset/train/malignant\"\n",
    "    shutil.copy(path, target_dir)\n",
    "for path, label in zip(val_paths, val_labels):\n",
    "    target_dir = \"/workspace/2024/vit/dataset/val/benign\" if label == 0 else \"/workspace/2024/vit/dataset/val/malignant\"\n",
    "    shutil.copy(path, target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cd2d5-5f11-4306-885d-872dfba01029",
   "metadata": {},
   "source": [
    "Data Transformation and DataLoader Setup : This block prepares the data pipeline for training and validation, with augmentation and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28716718-2f52-4adc-b1d6-26395ba2844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['benign', 'malignant'], Class to Index: {'benign': 0, 'malignant': 1}\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Higher resolution for better performance\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader with optimization\n",
    "train_dataset = datasets.ImageFolder(root=\"/workspace/2024/vit/dataset/train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"/workspace/2024/vit/dataset/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    pin_memory=True, \n",
    "    prefetch_factor=2, \n",
    "    persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Print the class names and mappings\n",
    "print(f\"Classes: {train_dataset.classes}, Class to Index: {train_dataset.class_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eeee10-a7dc-4cf5-b051-fe6856160b34",
   "metadata": {},
   "source": [
    "Define the Training Function : This block defines the train_model function, which handles the training, validation, and early stopping logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104c9307-dd22-40a6-964a-28235acd03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping and checkpointing\n",
    "def train_model(model, train_loader, val_loader, epochs=50, patience=5, checkpoint_dir=\"/workspace/2024/vit/checkpoints\"):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')  # To track the best validation loss\n",
    "    epochs_without_improvement = 0  # To track patience\n",
    "    best_model_wts = None  # To save the best model weights\n",
    "\n",
    "    # Ensure checkpoint directory exists\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()  # Start time for this epoch\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision context if using GPU\n",
    "            with autocast(enabled=use_amp):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            # Scale gradients and accumulate for larger batch sizes\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Update weights every `accumulation_steps`\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += targets.size(0)\n",
    "            correct_train += (predicted == targets).sum().item()\n",
    "\n",
    "        # Compute average training loss and accuracy for the epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Check if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_wts = model.state_dict()  # Save the best model weights\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "        # Early stopping if no improvement for `patience` epochs\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "    # Load best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(\"Restored the best model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06ba2a-4eea-489f-b229-557b5549dd91",
   "metadata": {},
   "source": [
    "Validation Function : This block contains the validate_model function that computes the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "322cf4b3-20e9-4828-93ec-fe3cca01b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606bd31-746c-4416-8392-0ec531b6c267",
   "metadata": {},
   "source": [
    "Delete Cache : Clears the CPU memory by deleting unused variables and calling garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "942b892a-7ad2-4323-bc78-5b55d88aefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cpu_memory():\n",
    "    gc.collect()  # Forces Python's garbage collection to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb4a4a-96b3-4928-afea-13fc55e546aa",
   "metadata": {},
   "source": [
    "Model Initialization and Training Loop : This block trains each model in sequence, applying Intel optimizations, training, quantization, and JIT scripting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd7fbf9-cfc5-4624-b341-15c9c980f2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_956/545539329.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2026, Training Accuracy: 93.12%\n",
      "Validation Loss: 0.1109, Validation Accuracy: 96.06%\n",
      "Epoch 1 completed in 356.15 seconds.\n",
      "Epoch 2, Training Loss: 0.1149, Training Accuracy: 96.27%\n",
      "Validation Loss: 0.1188, Validation Accuracy: 96.29%\n",
      "Epoch 2 completed in 354.67 seconds.\n",
      "Epoch 3, Training Loss: 0.0919, Training Accuracy: 97.02%\n",
      "Validation Loss: 0.0907, Validation Accuracy: 97.37%\n",
      "Epoch 3 completed in 358.68 seconds.\n",
      "Epoch 4, Training Loss: 0.0998, Training Accuracy: 96.89%\n",
      "Validation Loss: 0.0868, Validation Accuracy: 97.35%\n",
      "Epoch 4 completed in 366.66 seconds.\n",
      "Epoch 5, Training Loss: 0.1069, Training Accuracy: 96.52%\n",
      "Validation Loss: 0.1132, Validation Accuracy: 96.35%\n",
      "Checkpoint saved at epoch 5.\n",
      "Epoch 5 completed in 390.29 seconds.\n",
      "Epoch 6, Training Loss: 0.0994, Training Accuracy: 96.93%\n",
      "Validation Loss: 0.0955, Validation Accuracy: 97.40%\n",
      "Epoch 6 completed in 397.12 seconds.\n",
      "Epoch 7, Training Loss: 0.0860, Training Accuracy: 97.33%\n",
      "Validation Loss: 0.0818, Validation Accuracy: 97.62%\n",
      "Epoch 7 completed in 429.54 seconds.\n",
      "Epoch 8, Training Loss: 0.1081, Training Accuracy: 96.35%\n",
      "Validation Loss: 0.1061, Validation Accuracy: 96.47%\n",
      "Epoch 8 completed in 483.01 seconds.\n",
      "Epoch 9, Training Loss: 0.1181, Training Accuracy: 96.08%\n",
      "Validation Loss: 0.1113, Validation Accuracy: 97.15%\n",
      "Epoch 9 completed in 572.68 seconds.\n",
      "Epoch 10, Training Loss: 0.0909, Training Accuracy: 97.02%\n",
      "Validation Loss: 0.1010, Validation Accuracy: 96.81%\n",
      "Checkpoint saved at epoch 10.\n",
      "Epoch 10 completed in 675.21 seconds.\n",
      "Epoch 11, Training Loss: 0.0969, Training Accuracy: 96.90%\n",
      "Validation Loss: 0.1184, Validation Accuracy: 96.13%\n",
      "Epoch 11 completed in 687.96 seconds.\n",
      "Epoch 12, Training Loss: 0.0957, Training Accuracy: 97.09%\n",
      "Validation Loss: 0.0974, Validation Accuracy: 97.24%\n",
      "Early stopping triggered after 12 epochs.\n",
      "Restored the best model.\n",
      "Training completed for vit_tiny_patch16_224.\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model initialization for vit_tiny_patch16_224\n",
    "vit_tiny = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=2)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "vit_tiny, optimizer = ipex.optimize(vit_tiny, optimizer=Adam(vit_tiny.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "vit_tiny.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(vit_tiny, train_loader, val_loader, epochs=50, patience=5)\n",
    "\n",
    "# # Post-training Quantization using Neural Compressor\n",
    "# print(\"Quantizing vit_tiny_patch16_224...\")\n",
    "# config = PostTrainingQuantConfig()\n",
    "# quantized_vit_tiny = quantization.fit(vit_tiny, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# # Save the quantized model\n",
    "# torch.save(quantized_vit_tiny.state_dict(), '/workspace/2024/image/quantized_model/vit_tiny_patch16_224_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "script_vit_tiny = torch.jit.script(vit_tiny)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(script_vit_tiny,'/workspace/2024/vit/scripted_model/vit_tiny_patch16_224_scripted.pt')\n",
    "\n",
    "print(\"Training completed for vit_tiny_patch16_224.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99691733-6f9c-48a3-ac12-943ff8cb6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vit_tiny.state_dict(), '/workspace/2024/vit/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd1f6b-b50d-42ee-b2ab-a8d15b785bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization for vit_small_patch16_224\n",
    "vit_small = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "vit_small, optimizer = ipex.optimize(vit_small, optimizer=Adam(vit_small.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "vit_small.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(vit_small, train_loader, val_loader, epochs=50, patience=5)\n",
    "\n",
    "# Post-training Quantization using Neural Compressor\n",
    "print(\"Quantizing vit_small_patch16_224...\")\n",
    "config = PostTrainingQuantConfig()\n",
    "quantized_vit_small = quantization.fit(vit_small, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_vit_small.state_dict(), '/workspace/2024/image/quantized_model/vit_small_patch16_224_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "quantized_vit_small = torch.jit.script(quantized_vit_small)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(quantized_vit_small, '/workspace/2024/image/scripted_model/vit_small_patch16_224_quantized_scripted.pt')\n",
    "\n",
    "print(\"Training and quantization completed for vit_small_patch16_224.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268882e-1894-474c-9e51-9ab5b7992071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization for vit_base_patch16_224\n",
    "vit_base = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "vit_base, optimizer = ipex.optimize(vit_base, optimizer=Adam(vit_base.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "vit_base.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(vit_base, train_loader, val_loader, epochs=50, patience=5)\n",
    "\n",
    "# Post-training Quantization using Neural Compressor\n",
    "print(\"Quantizing vit_base_patch16_224...\")\n",
    "config = PostTrainingQuantConfig()\n",
    "quantized_vit_base = quantization.fit(vit_base, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_vit_base.state_dict(), '/workspace/2024/image/quantized_model/vit_base_patch16_224_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "quantized_vit_base = torch.jit.script(quantized_vit_base)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(quantized_vit_base, '/workspace/2024/image/scripted_model/vit_base_patch16_224_quantized_scripted.pt')\n",
    "\n",
    "print(\"Training and quantization completed for vit_base_patch16_224.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b57c8-aa24-4477-a3ca-d92b07fb0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization for convnextv2_nano\n",
    "convnextv2 = timm.create_model('convnextv2_nano', pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Use Intel optimizations for better CPU performance (IPEX)\n",
    "convnextv2, optimizer = ipex.optimize(convnextv2, optimizer=Adam(convnextv2.parameters(), lr=0.001), dtype=torch.float32)\n",
    "\n",
    "# Move model to device\n",
    "convnextv2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(convnextv2, train_loader, val_loader, epochs=50, patience=5)\n",
    "\n",
    "# Post-training Quantization using Neural Compressor\n",
    "print(\"Quantizing convnextv2_nano...\")\n",
    "config = PostTrainingQuantConfig()\n",
    "quantized_convnextv2 = quantization.fit(convnextv2, config=config, calib_dataloader=train_loader)\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_convnextv2.state_dict(), '/workspace/2024/image/quantized_model/convnextv2_nano_quantized.pth')\n",
    "\n",
    "# Apply JIT scripting for deployment\n",
    "quantized_convnextv2 = torch.jit.script(quantized_convnextv2)\n",
    "\n",
    "# Save the scripted model\n",
    "torch.jit.save(quantized_convnextv2, '/workspace/2024/image/scripted_model/convnextv2_nano_quantized_scripted.pt')\n",
    "\n",
    "print(\"Training and quantization completed for convnextv2_nano.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b94cad-cfbc-4b99-88f8-0ab5f892d47d",
   "metadata": {},
   "source": [
    "Final Message : This final block confirms that all models have been trained and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619199c4-1e5c-4748-9d8d-fb1d0cf3d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "print(\"All models trained, quantized, and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
